---
title: "Lab3 - real-time"
linkTitle: "Lab3 - real-time"
weight: 3
description: >
  Build an app to upsert real-time data to TIBCO Graph Database
---

Now let's create an application which and receive real-time order events from Kafka topic then build graph entities, upsert to TGDB then serve as real-time streaming server.

First of all create a "Server Sent Event" internal connection to bridge the data between order event flow and server (for serving streaming graph entity to client).

In "Connections" tab select GraphBuilder_SSE -> Server-sent Events Connection

![Realtime](realtime02.png)

Connection settings (following settings match the client tool provided for browsing real-time graph entity update)
- Connection Name : Set name to "EventServer"
- Outbound : Set false as it's a server
- Server port : 8888
- Path : It's URI path "/sse/"
- TLS enabled : false

Click connect

![Realtime](realtime01.png)

Back to Northwind application to create a new flow called "Order Event Server"

![Realtime](realtime07.png)

Create a trigger to serve graph entities (generated by order event) for streaming client

![Realtime](realtime03.png)

Settings
- Connection Name : Select the "EventServer" connection which we just created

Click save

![Realtime](realtime04.png)

This simple flow will be serving streaming graph entities

![Realtime](realtime04-5.png)

The last flow in Northwind application is "Order Data Flow" which listen to Kafla topic to get order event as input data. 

Before we create the flow, we need to create a "Kafka Connection". In connection tab select "Appach Kafka Client Configuration"  

![Realtime](realtime06.png)

Configure Appach Kafka Client as following screenshot then save it

![Realtime](realtime05.png)

Back to application to create new flow called "Order Event"

![Realtime](realtime08.png)

Click Flow Inputs & Outputs (vertical blue bar) to define schema between flow and trigger. Set following data sample then click save

![Realtime](realtime12.png)

After click save button schema generator would convert sample data to schema definition

![Realtime](realtime13.png)

Click "+" to add trigger (Kafka Vonsumer) 

![Realtime](realtime09.png)

Select "Northwind Orders" configuration we just created then click continue.

![Realtime](realtime10.png)

Select "Just Add Trigger" button to add trigger

![Realtime](realtime11.png)

Filling trigger setting as it shown below in screenshot

![Realtime](realtime14.png)

Map OrderString to $trigger.stringValue

![Realtime](realtime15.png)

Add CSVParser to convert incoming CVS string to system object

![Realtime](realtime16.png)

Follow the instruction in lab1 define the mapping between CSV fields and attribute of system object. Use the column field name as attribute name.

Make sure "First Row Is Header" set to false

![Realtime](realtime17.png)

Configure the input
- CSVString : $flow.OrderString
- Leave SequenceNumber not mapped

![Realtime](realtime18.png)

After the data has bean transform to the object which could be recognized by the system. The next step is to convert data to graph entities (nodes, edges and attributes). We use the core activity "Build Graph" to perform this tranformation.
Let's select GraphBuilder -> Bruild Graph and configue it.

![Realtime](realtime18-5.png)

Follow lab1's instruction to turn on the iterator to iterate through upstream output data then map the input data. Here is the mapping

Product node
- ProductID -> $iteration[value].ProductID

Employee node
- EmployeeID -> $iteration[value].EmployeeID

Customer node
- CustomerID -> $iteration[value].CustomerID

Order node
- OrderID -> $iteration[value].OrderID
- CustomerID -> $iteration[value].CustomerID
- EmployeeID- > $iteration[value].EmployeeID
- OrderDate -> $iteration[value].OrderDate
- RequiredDate -> $iteration[value].RequiredDate
- ShippedDate -> $iteration[value].ShippedDate
- ShipVia -> $iteration[value].ShipVia
- Freight -> $iteration[value].Freight
- ShipName -> $iteration[value].ShipName
- ShipAddress -> $iteration[value].ShipAddress
- ShipCity -> $iteration[value].ShipCity
- ShipRegion -> $iteration[value].ShipRegion
- ShipPostalCode - > $iteration[value].ShipPostalCode
- ShipCountry -> $iteration[value].ShipCountry

Suborder node
- OrderID -> $iteration[value].OrderID
- ProductID -> $iteration[value].ProductID
- UnitPrice -> $iteration[value].UnitPrice
- Quantity -> $iteration[value].Quantity
- Discount -> $iteration[value].Discount

Region node
- RegionName -> $iteration[value].RegionName
- Country -> $iteration[value].Country

Since one order can be splited to multiple order events (with different product sold). We create two types of order nodes 1. Odrer node with OrderID as its primary key and 2. Suborder node with OrderID, ProductID as primary key. All Suborder nodes connect to Order node by OrderID. (See following screenshot)

Order : 

![Realtime](realtime19.png)

Suborder : 

![Realtime](realtime20.png)

Follow lab1's instruction to add TGDBUpsert activity

![Realtime](realtime20-2.png)

Select Connetion

![Realtime](realtime20-3.png)

Map input data

![Realtime](realtime20-4.png)

Now adding a new type of activity called SSEEndPoint which sends graph entities to SSEServer for serving streaming client.

Select SSEEndPoint activity from GraphBuilder_SSE.

![Realtime](realtime21.png)

Select "SSEConnection" we created and used in SSEServer then SSEEndPoint is connected to SSEServer now.

![Realtime](realtime22.png)

Setup SessionId to "order" so the complete URI to access to this event flow would be /sse/order

![Realtime](realtime23.png)

Map input data to Graph object from BuildGraph activity

![Realtime](realtime24.png)

We can add log and GraphtoFile activities like previous configured flow.

![Realtime](realtime25.png)

Now we have finish last flow for Northwind application.

![Realtime](realtime26.png)

This is the final version of flogo Northwind application 

![Realtime](realtime27.png)
