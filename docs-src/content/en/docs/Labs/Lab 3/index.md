---
title: "Lab3 - real-time"
linkTitle: "Lab3 - real-time"
weight: 3
description: >
  Build an app to upsert real-time data to TIBCO Graph Database
---

Now let's create an application which and receive real-time order events from Kafka topic then build graph entities, upsert to TGDB then serve as real-time streaming server.

First of all create a "Server Sent Event" internal connection to bridge the data between order event flow and server (for serving streaming graph entity to client).

In "Connections" tab select GraphBuilder_SSE -> Server-sent Events Connection

![Realtime](realtime02.png)

Connection settings (following settings match the client tool provided for browsing real-time graph entity update)
- Connection Name : Set name to "EventServer"
- Outbound : Set false as it's a server
- Server port : 8888
- Path : It's URI path "/sse/"
- TLS enabled : false

Click connect

![Realtime](realtime01.png)

Back to Northwind application to create a new flow called "Order Event Server"

![Realtime](realtime07.png)

Create a trigger to serve graph entities (generated by order event) for streaming client

![Realtime](realtime03.png)

Settings
- Connection Name : Select the "EventServer" connection which we just created

Click save

![Realtime](realtime04.png)

This simple flow will be serving streaming graph entities

![Realtime](realtime04-5.png)

The last flow in Northwind application is "Order Data Flow" which listen to Kafla topic to get order event as input data. 

Before we create the flow, we need to create a "Kafka Connection". In connection tab select "Appach Kafka Client Configuration"  

![Realtime](realtime06.png)

Configure Appach Kafka Client as following screenshot then save it

![Realtime](realtime05.png)

Back to application to create new flow called "Order Event"

![Realtime](realtime08.png)

Click Flow Inputs & Outputs (vertical blue bar) to define schema between flow and trigger. Set following data sample then click save

![Realtime](realtime12.png)

After click save button schema generator would convert sample data to schema definition

![Realtime](realtime13.png)

Click "+" to add trigger (Kafka Vonsumer) 

![Realtime](realtime09.png)

Select "Northwind Orders" configuration we just created then click continue.

![Realtime](realtime10.png)

Select "Just Add Trigger" button to add trigger

![Realtime](realtime11.png)

Filling trigger setting as it shown below in screenshot

![Realtime](realtime14.png)

Map OrderString to $trigger.stringValue

![Realtime](realtime15.png)

Add CSVParser to convert incoming CVS string to system object

![Realtime](realtime16.png)

Follow the instruction in lab1 define the mapping between CSV fields and attribute of system object. Use the column field name as attribute name.

Make sure "First Row Is Header" set to false

![Realtime](realtime17.png)

Configure the input
- CSVString : $flow.OrderString
- Leave SequenceNumber not mapped

![Realtime](realtime18.png)

After the data has bean transform to the object which could be recognized by the system. The next step is to convert data to graph entities (nodes, edges and attributes). We use the core activity "Build Graph" to perform this tranformation.
Let's select GraphBuilder -> Bruild Graph and configue it.

![Realtime](realtime18-5.png)

Follow lab1's instruction to turn on the iterator to iterate through upstream output data then map the input data. Here is the mapping

Product node
- ProductID -> $iteration[value].ProductID

Employee node
- EmployeeID -> $iteration[value].EmployeeID

Customer node
- CustomerID -> $iteration[value].CustomerID

Order node
- OrderID -> $iteration[value].OrderID
- CustomerID -> $iteration[value].CustomerID
- EmployeeID- > $iteration[value].EmployeeID
- OrderDate -> $iteration[value].OrderDate
- RequiredDate -> $iteration[value].RequiredDate
- ShippedDate -> $iteration[value].ShippedDate
- ShipVia -> $iteration[value].ShipVia
- Freight -> $iteration[value].Freight
- ShipName -> $iteration[value].ShipName
- ShipAddress -> $iteration[value].ShipAddress
- ShipCity -> $iteration[value].ShipCity
- ShipRegion -> $iteration[value].ShipRegion
- ShipPostalCode - > $iteration[value].ShipPostalCode
- ShipCountry -> $iteration[value].ShipCountry

Suborder node
- OrderID -> $iteration[value].OrderID
- ProductID -> $iteration[value].ProductID
- UnitPrice -> $iteration[value].UnitPrice
- Quantity -> $iteration[value].Quantity
- Discount -> $iteration[value].Discount

Region node
- RegionName -> $iteration[value].RegionName
- Country -> $iteration[value].Country

Since one order can be splited to multiple order events (with different product sold). We create two types of order nodes 1. Odrer node with OrderID as its primary key and 2. Suborder node with OrderID, ProductID as primary key. All Suborder nodes connect to Order node by OrderID. (See following screenshot)

Order : 

![Realtime](realtime19.png)

Suborder : 

![Realtime](realtime20.png)

Follow lab1's instruction to add TGDBUpsert activity

![Realtime](realtime20-2.png)

Select Connetion

![Realtime](realtime20-3.png)

Map input data

![Realtime](realtime20-4.png)

Now adding a new type of activity called SSEEndPoint which sends graph entities to SSEServer for serving streaming client.

Select SSEEndPoint activity from GraphBuilder_SSE.

![Realtime](realtime21.png)

Select "SSEConnection" we created and used in SSEServer then SSEEndPoint is connected to SSEServer now.

![Realtime](realtime22.png)

Setup SessionId to "order" so the complete URI to access to this event flow would be /sse/order

![Realtime](realtime23.png)

Map input data to Graph object from BuildGraph activity

![Realtime](realtime24.png)

We can add log and GraphtoFile activities like previous configured flow.

![Realtime](realtime25.png)

Now we have finish last flow for Northwind application.

![Realtime](realtime26.png)

This is the final version of flogo Northwind application 

![Realtime](realtime27.png)

Let's rebuild application for further testing

![Realtime](BuildNorthwind_02.png)

We are going to install Kafka message bus for providing order event. <a href="https://kafka.apache.org/quickstart" target="_blank">Here</a> is the instalation instructions.

After downloading and installing Kafka we can start Kafka
- Start zoo keeper

![Realtime](StartKafka01.png)
![Realtime](StartKafka02.png)

- Start server

![Realtime](StartKafka03.png)

- Create "test" topic

![Realtime](StartKafka04.png)

Let's restart Northwind appliction executable. 
- Switch to the folder which contains Northwind appliction executable (Northwind-darwin_amd64). 
- Change Northwind-darwin_amd64's permission to executable 
- Run Northwind-darwin_amd64

This time you'll see two extra information while Northwind application starting
- Kafka consumer (the trigger of order event flow) is up and listening
- SSEServer is up and waiting for client (UI utility) to connect 

![Realtime](StartKafka05.png)

Here it the our test (see screenshot)
- Make sure TGDB, TGDB_RESTful_Service, Kafka (server, zoo keeper, producer) and UI utility are running 
- On the upper/middle left of screenshot open oerders.csv file 
- On the lower left of screenshot start Kafka producer and keep it opened
- On the right follow the instruction to 1. Click "Realtime Data" 2. Click "Connect" to connect to SSE server in Northwind application 3. Copy & paste order to Kafka producer then press enter 4 ~ 6. Each time you send one order you will see the corresponding graph entities showing on the UI.

Send more order as your wish. 

![Realtime](FinalTest.png)

After the streaming testing we also want to see the order in TGDB. Follow the instrctions in lab2
- Click "TGDB Data" button
- Use the default query setup but make traversalDepth = 5
- Click "Make Query" button

You'll see the oder with OrderID = 10248 and its associated graph entities on the UI

![Realtime](QueryGraph.png)

The last test is about traversal query. We are going to find the supplier companies which supplies prodcut which the customer company called 'Vins et alcools Chevalier' purchased. We are going use Postman and TGDB_RESTful_Service to query against TGDB server
- Open a postman and setup a POST query
- The gremlin query is "g.V().has('Company', 'CompanyID', 'Vins et alcools Chevalier').in('Customer_Company').in('SoldTo').out('Includes').out('Contains').in('Supplies').out('Supplier_Company');"

You should get "Formaggi Fortini s.r.l.", "Leka Trading" and "Cooperativa de Quesos 'Las Cabras'" in your result

![Realtime](QueryPostMan.png)

Observe the traversal request on the UI utility and verify the correctness ogf the query

![Realtime](QueryPostManOnGraph.png)

Now you've finish all three labs